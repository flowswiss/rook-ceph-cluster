# Values for FRESH INSTALLATION (new cluster)
# Use this for completely new Rook Ceph deployments

# Global settings
global:
  imageRegistry: ""
  imagePullSecrets: []

# Name overrides
nameOverride: ""
fullnameOverride: ""

# Rook Operator Configuration
operator:
  # Latest Rook version
  image:
    repository: rook/ceph
    tag: v1.17.4
    pullPolicy: IfNotPresent
  
  namespace: rook-ceph
  
  # k0s specific environment variables
  env:
    ROOK_CSI_KUBELET_DIR_PATH: "/var/lib/k0s/kubelet"
    ROOK_CSI_CEPH_IMAGE: "quay.io/cephcsi/cephcsi:v3.12.2"
    ROOK_CSI_REGISTRAR_IMAGE: "registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.11.1"
    ROOK_CSI_RESIZER_IMAGE: "registry.k8s.io/sig-storage/csi-resizer:v1.11.1"
    ROOK_CSI_PROVISIONER_IMAGE: "registry.k8s.io/sig-storage/csi-provisioner:v5.0.1"
    ROOK_CSI_SNAPSHOTTER_IMAGE: "registry.k8s.io/sig-storage/csi-snapshotter:v8.0.1"
    ROOK_CSI_ATTACHER_IMAGE: "registry.k8s.io/sig-storage/csi-attacher:v4.6.1"
  
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi
  
  # Scheduling
  nodeSelector: {}
  tolerations: []
  affinity: {}
  
  # CSI Driver Configuration
  csi:
    enableRBD: true
    enableCephFS: true
    enableSnapshotter: true
    pluginTolerations: []
    provisionerTolerations: []
    logLevel: 0
  
  # Discovery daemon for device detection
  enableDiscoveryDaemon: true

# Ceph Cluster Configuration  
cephCluster:
  # Latest Ceph version (Squid LTS)
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.3
    allowUnsupported: false
  
  dataDirHostPath: /var/lib/rook
  
  # Dashboard Configuration
  dashboard:
    enabled: true
    port: 8080
    ssl: false

  # Monitor configuration (3 for HA)
  mon:
    count: 3
    allowMultiplePerNode: false
  
  # Manager configuration (2 for HA)
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true
      - name: rook
        enabled: true

  # Crash collector and log collector
  crashCollector:
    disable: false
  
  logCollector:
    enabled: true
    periodicity: daily
  
  cleanupPolicy:
    confirmation: ""
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
    allowUninstallWithVolumes: false
  
  # Storage configuration for your 3-node cluster
  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter: "^sdb"
    config:
      osdsPerDevice: "1"
    # Your specific nodes and devices
    nodes:
      - name: my-001-bit1
        devices:
          - name: sdb
      - name: my-002-bit1
        devices:
          - name: sdb
      - name: my-003-bit1
        devices:
          - name: sdb
  
  # Node placement for your specific nodes
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
              - my-001-bit1
              - my-002-bit1
              - my-003-bit1

  # Resource allocation optimized for k0s
  resources:
    mgr:
      requests:
        cpu: 500m
        memory: 1Gi
    mon:
      requests:
        cpu: 500m
        memory: 1Gi
    osd:
      requests:
        cpu: 500m
        memory: 2Gi

  # Priority classes for critical workloads
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical

  # Health checks
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mgr:
        disabled: false
      mon:
        disabled: false
      osd:
        disabled: false
  
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
  
  network:
    connections:
      compression:
        enabled: false
  
  security:
    keyRotation:
      enabled: false

# Block storage pool configuration
cephBlockPools:
  - name: replicapool
    spec:
      failureDomain: host
      replicated:
        size: 2  # 2 replicas for 3-node cluster
    storageClass:
      enabled: true
      name: rook-ceph-block
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

# Filesystem Configuration (shared storage)
cephFileSystems:
  - name: myfs
    spec:
      metadataPool:
        replicated:
          size: 2
      dataPools:
        - name: data0
          replicated:
            size: 2
      preserveFilesystemOnDelete: true
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
    storageClass:
      enabled: true
      name: rook-cephfs
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
        pool: myfs-replicated

# Toolbox for debugging and management
toolbox:
  enabled: true
  image: quay.io/ceph/ceph:v19.2.3
  tolerations: []
  affinity: {}
  resources:
    limits:
      cpu: 500m
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 128Mi
