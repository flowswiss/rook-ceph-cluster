# ğŸš€ Rook Ceph Helm Chart fÃ¼r k0s

## Komplette Rook Ceph Installation fÃ¼r k0s Kubernetes

Dieses Helm Chart installiert den **kompletten Rook Ceph Stack** auf frischen k0s Kubernetes-Clustern:

- âœ… **CRDs** (Custom Resource Definitions)
- âœ… **Rook Operator** (rook-ceph-operator)
- âœ… **CSI Driver** (RBD + CephFS)
- âœ… **Ceph Cluster** (3-Node HA)
- âœ… **Storage Classes** (Block + FileSystem)
- âœ… **RBAC** (Alle benÃ¶tigten Berechtigungen)

## ğŸ¯ Ein-Kommando-Installation

```bash
# Komplette Installation mit automatischem CRD-Management
./deploy.sh

# Oder manuell in 2 Schritten:
kubectl apply -f crds/crds.yaml
helm install rook-ceph-cluster . -n rook-ceph --create-namespace
```

## ğŸ“‹ Voraussetzungen

- **k0s Kubernetes**: v1.28+ (empfohlen: v1.33+)
- **Helm**: v3.8+
- **Raw Storage**: `/dev/sdb` auf allen Worker Nodes
- **Worker Nodes**: Mindestens 3 Nodes fÃ¼r HA
- **Node Names**: `my-001-bit1`, `my-002-bit1`, `my-003-bit1` (anpassbar)

## ğŸ—ï¸ Architektur

### **Generierte Kubernetes Ressourcen:**

| Komponente | Beschreibung | Anzahl |
|------------|-------------|--------|
| **CRDs** | Custom Resource Definitions (separat) | 15 |
| **Operator** | Rook Ceph Operator | 1 |
| **ServiceAccounts** | RBAC Service Accounts | 9 |
| **ClusterRoles** | Cluster-Berechtigungen | 3 |
| **CSI Components** | RBD + CephFS Drivers | 4 |
| **Ceph Cluster** | Haupt-Cluster-Ressource | 1 |
| **Storage Pools** | Block + Filesystem Pools | 2 |
| **StorageClasses** | Kubernetes Storage Classes | 2 |
| **Services** | Ceph Services | 2 |
| **ConfigMaps** | CSI Konfiguration | 2 |

### **Ceph Cluster Layout:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   my-001-bit1   â”‚   my-002-bit1   â”‚   my-003-bit1   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MON + MGR + OSD â”‚ MON + MGR + OSD â”‚ MON + OSD       â”‚
â”‚ /dev/sdb        â”‚ /dev/sdb        â”‚ /dev/sdb        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation

### **Methode 1: Automatisches Script (empfohlen)**

```bash
# Einfache Installation mit CRD-Management
./deploy.sh
```

### **Methode 2: Manuell in 2 Schritten**

```bash
# Schritt 1: CRDs installieren
kubectl apply -f crds/crds.yaml

# Schritt 2: Helm Chart installieren
helm install rook-ceph-cluster . \
  --namespace rook-ceph \
  --create-namespace \
  --wait \
  --timeout=20m

# Status prÃ¼fen
helm status rook-ceph-cluster -n rook-ceph
```

### **Methode 3: Mit angepassten Values**

```bash
# CRDs und Chart mit angepassten Values
kubectl apply -f crds/crds.yaml
helm install rook-ceph-cluster . \
  -n rook-ceph \
  --create-namespace \
  -f custom-values.yaml
```

## âš™ï¸ Konfiguration

### **Wichtige Values in `values.yaml`:**

```yaml
# Operator Konfiguration
operator:
  image:
    tag: v1.17.4
  namespace: rook-ceph

# Ceph Cluster
cephCluster:
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.3
  
  # k0s-spezifischer Storage
  storage:
    deviceFilter: "^sdb"  # Alle /dev/sdb Devices
    nodes:
      - name: my-001-bit1
        devices:
          - name: sdb
      # Weitere Nodes...

  # HA Konfiguration
  mon:
    count: 3  # 3 Monitore fÃ¼r HA
  mgr:
    count: 2  # 2 Manager fÃ¼r HA

# Storage Pools
cephBlockPools:
  - name: replicapool
    spec:
      replicated:
        size: 2  # 2 Replicas bei 3 Nodes

cephFileSystems:
  - name: myfs
    spec:
      metadataPool:
        replicated:
          size: 2
```

## ğŸ“Š Nach der Installation

### **Cluster Status prÃ¼fen:**

```bash
# Helm Status
helm status rook-ceph-cluster -n rook-ceph

# Ceph Cluster Status
kubectl get cephcluster -n rook-ceph

# Alle Pods
kubectl get pods -n rook-ceph

# Ceph Health (nach ~5-10 Minuten)
kubectl -n rook-ceph exec deploy/rook-ceph-cluster-tools -- ceph status
```

### **Storage Classes verwenden:**

```bash
# VerfÃ¼gbare Storage Classes
kubectl get storageclass

# Block Storage verwenden
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: rook-ceph-block

# Shared Storage verwenden
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-shared-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: rook-cephfs
```

## ğŸ”§ Management

### **Cluster-Operationen:**

```bash
# Upgrade
helm upgrade rook-ceph-cluster . -n rook-ceph

# Konfiguration anzeigen
helm get values rook-ceph-cluster -n rook-ceph

# Deinstallation (ACHTUNG: LÃ¶scht alle Daten!)
helm uninstall rook-ceph-cluster -n rook-ceph
```

### **Ceph-Management:**

```bash
# Ceph Status
kubectl -n rook-ceph exec deploy/rook-ceph-cluster-tools -- ceph status

# OSD Status
kubectl -n rook-ceph exec deploy/rook-ceph-cluster-tools -- ceph osd status

# Pool Information
kubectl -n rook-ceph exec deploy/rook-ceph-cluster-tools -- ceph df
```

## ğŸ› ï¸ Anpassungen

### **Andere Node-Namen:**

In `values.yaml` unter `cephCluster.storage.nodes` anpassen:

```yaml
storage:
  nodes:
    - name: worker-1
      devices:
        - name: sdb
    - name: worker-2
      devices:
        - name: sdb
```

### **Andere Storage-Devices:**

```yaml
storage:
  deviceFilter: "^nvme"  # FÃ¼r NVMe SSDs
  nodes:
    - name: my-001-bit1
      devices:
        - name: nvme0n1
```

### **Ressourcen-Limits anpassen:**

```yaml
cephCluster:
  resources:
    osd:
      requests:
        cpu: 1000m
        memory: 4Gi
      limits:
        cpu: 2000m
        memory: 8Gi
```

## â— Wichtige Hinweise

- **Daten-Verlust**: Deinstallation lÃ¶scht alle Ceph-Daten unwiderruflich
- **Storage-Devices**: MÃ¼ssen roh/unformatiert sein (`/dev/sdb` darf keine Partitionen haben)
- **Network**: Alle Nodes mÃ¼ssen untereinander kommunizieren kÃ¶nnen
- **Zeit**: Erste Installation kann 10-15 Minuten dauern

## ğŸ” Troubleshooting

### **Cluster startet nicht:**

```bash
# Operator Logs
kubectl logs -n rook-ceph deployment/rook-ceph-operator

# CephCluster Status
kubectl describe cephcluster rook-ceph -n rook-ceph
```

### **OSDs werden nicht erstellt:**

```bash
# OSD Logs
kubectl logs -n rook-ceph -l app=rook-ceph-osd-prepare

# Device-Discovery
kubectl logs -n rook-ceph -l app=rook-discover
```

### **Storage Classes funktionieren nicht:**

```bash
# CSI Logs
kubectl logs -n rook-ceph -l app=csi-rbdplugin
kubectl logs -n rook-ceph -l app=csi-cephfsplugin
```

---

**FÃ¼r Support und weitere Informationen siehe:** [Rook Documentation](https://rook.io/docs/)
