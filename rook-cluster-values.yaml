# Values für Rook Ceph Cluster Helm Chart
# Installation: helm install --namespace rook-ceph rook-ceph-cluster rook-release/rook-ceph-cluster -f rook-ceph-cluster-values.yaml

# Namespace für den Cluster
operatorNamespace: rook-ceph

# Cluster Configuration
cephClusterSpec:
  # Ceph Version
  cephVersion:
    image: quay.io/ceph/ceph:v19.2.3
    allowUnsupported: false
  
  # Data Directory auf den Hosts
  dataDirHostPath: /var/lib/rook
  
  # Cluster Management
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  waitTimeoutForHealthyOSDInMinutes: 10
  
  # Mon Configuration
  mon:
    count: 3
    allowMultiplePerNode: false
  
  # Mgr Configuration  
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      - name: pg_autoscaler
        enabled: true
      - name: rook
        enabled: true
  
  # Dashboard Configuration
  dashboard:
    enabled: true
    port: 8080
    ssl: false
    urlPrefix: /
  
  # Network Configuration
  network:
    connections:
      compression:
        enabled: false
  
  # Crash Collector
  crashCollector:
    disable: false
  
  # Log Collector
  logCollector:
    enabled: true
    periodicity: daily
  
  # Cleanup Policy
  cleanupPolicy:
    allowUninstallWithVolumes: false
    confirmation: ""
    sanitizeDisks:
      method: quick
      dataSource: zero
      iteration: 1
  
  # Health Check Configuration
  healthCheck:
    daemonHealth:
      mon:
        disabled: false
        interval: 45s
      osd:
        disabled: false
        interval: 60s
      status:
        disabled: false
        interval: 60s
    livenessProbe:
      mon:
        disabled: false
      mgr:
        disabled: false
      osd:
        disabled: false
  
  # Storage Configuration
  storage:
    useAllNodes: true
    useAllDevices: false
    deviceFilter: "^sdb"
    config:
      osdsPerDevice: "1"
    nodes:
      - name: my-001-bit1
        devices:
          - name: sdb
      - name: my-002-bit1
        devices:
          - name: sdb
      - name: my-003-bit1
        devices:
          - name: sdb
  
  # Disruption Management
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    pgHealthCheckTimeout: 0
  
  # Resource Requests/Limits
  resources:
    mgr:
      requests:
        cpu: 500m
        memory: 1Gi
    mon:
      requests:
        cpu: 500m
        memory: 1Gi
    osd:
      requests:
        cpu: 500m
        memory: 2Gi
  
  # Priority Class Names
  priorityClassNames:
    mon: system-node-critical
    osd: system-node-critical
    mgr: system-cluster-critical
  
  # Placement Configuration für alle Nodes
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: kubernetes.io/hostname
                  operator: In
                  values:
                    - my-001-bit1
                    - my-002-bit1
                    - my-003-bit1
  
  # Security Settings
  security:
    keyRotation:
      enabled: false

# Block Pool Configuration
cephBlockPools:
  - name: replicapool
    spec:
      failureDomain: host
      replicated:
        size: 2
    storageClass:
      enabled: true
      name: rook-ceph-block
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        imageFormat: "2"
        imageFeatures: layering
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

# File System Configuration
cephFileSystems:
  - name: myfs
    spec:
      metadataPool:
        replicated:
          size: 2
      dataPools:
        - name: data0
          replicated:
            size: 2
      metadataServer:
        activeCount: 1
        activeStandby: true
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
      preserveFilesystemOnDelete: true
    storageClass:
      enabled: true
      name: rook-cephfs
      isDefault: false
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: Immediate
      parameters:
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

# Object Store Configuration (deaktiviert, da nicht in Ihrem Cluster vorhanden)
cephObjectStores: []

# Block Pool Configuration für CSI
cephBlockPoolsVolumeSnapshotClass:
  enabled: false

# Monitoring (deaktiviert, da nicht in Ihrem Cluster konfiguriert)
monitoring:
  enabled: false
  createPrometheusRules: false

# Ingress Configuration (optional)
ingress:
  dashboard:
    enabled: false

# Toolbox Deployment (praktisch für Debugging)
toolbox:
  enabled: true
  image: quay.io/ceph/ceph:v19.2.3
  resources:
    requests:
      cpu: 100m
      memory: 128Mi

# Cleanup on Uninstall
cleanupPolicyConfirmation: ""